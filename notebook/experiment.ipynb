{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact-Checking AI Agent Experiment\n",
    "\n",
    "This notebook implements an AI agent that extracts claims from a PDF and verifies them against a set of web sources using a RAG (Retrieval-Augmented Generation) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66fcaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/suriyaa/Desktop/check-mate\n",
      "Loaded 10 sources from data/sources.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Setup Paths\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utils.model_loader import ModelLoader\n",
    "from data.sources import sources\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Loaded {len(sources)} sources from data/sources.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-16T15:06:44.909695Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-16T15:06:44.910480Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-16T15:06:44.910889Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_9p...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-16T15:06:44.911395Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"faiss_db\", \"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-16T15:06:44.914310Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-16T15:06:44.914770Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2026-01-16T15:06:44.950730Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Models\n",
    "loader = ModelLoader()\n",
    "llm = loader.load_llm()\n",
    "embeddings = loader.load_embeddings()\n",
    "print(\"LLM and Embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: /Users/suriyaa/Desktop/check-mate/data/Donald Trump’s Second Term.pdf\n",
      "PDF Text Length: 2733 characters\n"
     ]
    }
   ],
   "source": [
    "# Load PDF Content\n",
    "pdf_path = os.path.join(project_root, \"data\", \"Donald Trump’s Second Term.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: PDF not found at {pdf_path}\")\n",
    "else:\n",
    "    print(f\"Loading PDF from: {pdf_path}\")\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    pdf_text = \"\\n\".join([doc.page_content for doc in pdf_docs])\n",
    "    print(f\"PDF Text Length: {len(pdf_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key claims from the PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Extracted Claims ---\n",
      "Here are the key, verifiable claims extracted from the text, focusing on Donald Trump's second term foreign policy:\n",
      "\n",
      "1.  In January 2026, the Trump administration formally announced the withdrawal of the United States from 66 international organizations, conventions, and treaties.\n",
      "\n",
      "Extracted 1 individual claims.\n"
     ]
    }
   ],
   "source": [
    "# Extract Claims from PDF\n",
    "print(\"Extracting key claims from the PDF...\")\n",
    "\n",
    "claim_extraction_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert fact-checker. Extract key claims from the following text that are specific, verifiable, and relevant to the topic of \"Donald Trump's Second Term\" foreign policy or actions.\n",
    "    Focus on assertions of fact rather than opinions.\n",
    "    Return the claims as a numbered list.\n",
    "    \n",
    "    Text:\n",
    "    {text}\n",
    "    \n",
    "    Key Claims:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "claim_chain = claim_extraction_prompt | llm | StrOutputParser()\n",
    "# Invoke with the full PDF text (assuming it fits in context, otherwise we'd chunk)\n",
    "claims_raw = claim_chain.invoke({\"text\": pdf_text})\n",
    "\n",
    "print(\"--- Raw Extracted Claims ---\")\n",
    "print(claims_raw)\n",
    "\n",
    "# Parse and clean claims\n",
    "claims = []\n",
    "for line in claims_raw.split('\\n'):\n",
    "    line = line.strip()\n",
    "    if line and (line[0].isdigit() or line.startswith('-')):\n",
    "        # Remove numbering/bullets e.g. \"1. \" or \"- \"\n",
    "        cleaned = re.sub(r'^[\\d\\-\\.\\s]+', '', line)\n",
    "        if cleaned:\n",
    "            claims.append(cleaned)\n",
    "\n",
    "print(f\"\\nExtracted {len(claims)} individual claims.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web sources...\n",
      "Loaded 10 web documents.\n",
      "Splitting documents and creating Vector Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "Loading faiss.\n",
      "Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store ready.\n"
     ]
    }
   ],
   "source": [
    "# Load and Index Web Sources (RAG Setup)\n",
    "print(\"Loading web sources...\")\n",
    "web_loader = WebBaseLoader(sources)\n",
    "web_docs = web_loader.load()\n",
    "print(f\"Loaded {len(web_docs)} web documents.\")\n",
    "\n",
    "print(\"Splitting documents and creating Vector Store...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "web_chunks = text_splitter.split_documents(web_docs)\n",
    "\n",
    "# Create FAISS Index\n",
    "vectorstore = FAISS.from_documents(web_chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Vector Store ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting verification process...\n",
      "\n",
      "Checking Claim 1/1\n",
      "Claim: In January 2026, the Trump administration formally announced the withdrawal of the United States from 66 international organizations, conventions, and treaties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "AFC is enabled with max remote calls: 10.\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Verdict**: SUPPORTED\n",
      "**Explanation**: The provided context directly supports the claim. The documents state that on January 7, 2026, President Trump announced the withdrawal of the United States from 66 international organizations, conventions, and treaties. Specifically, one document mentions \"President Donald Trump, citing 'the authority vested in me as President by the Constitution and the laws of the United States of America,' withdrew the United States 'from International Organizations, Conventions, and Treaties that Are Contrary to the Interests of the United States.'\" Another document mentions \"On January 7, 2026, through Executive Order 14199, President Trump announced the withdrawal of the United States from 66 international organizations (35 non-United Nations and 31 UN.\"\n",
      "--------------------------------------------------\n",
      "Verification Complete.\n"
     ]
    }
   ],
   "source": [
    "# Fact-Check Claims\n",
    "print(\"Starting verification process...\\n\")\n",
    "\n",
    "fact_check_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a strict fact-checker. Verify the following claim based ONLY on the provided context retrieved from reliable sources.\n",
    "    \n",
    "    Claim: {claim}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Determine if the claim is SUPPORTED, CONTRADICTED, or NOT_MENTIONED by the context.\n",
    "    Provide a brief explanation citing specific parts of the context.\n",
    "    \n",
    "    Output Format:\n",
    "    **Verdict**: [SUPPORTED / CONTRADICTED / NOT_MENTIONED]\n",
    "    **Explanation**: [Your explanation here]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "verification_chain = (\n",
    "    {\"context\": retriever, \"claim\": RunnablePassthrough()}\n",
    "    | fact_check_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, claim in enumerate(claims):\n",
    "    print(f\"Checking Claim {i+1}/{len(claims)}\")\n",
    "    print(f\"Claim: {claim}\")\n",
    "    \n",
    "    try:\n",
    "        result = verification_chain.invoke(claim)\n",
    "        print(result)\n",
    "        results.append({\"claim\": claim, \"verification\": result})\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying claim: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Verification Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd19fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import PromptLoader\n",
    "from prompt.prompt_library import PROMPT_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cb402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\n    You are an expert fact-checker. Extract key claims from the following text that are specific, verifiable, and relevant to the topic of \"Donald Trump\\'s Second Term\" foreign policy or actions.\\n    Focus on assertions of fact rather than opinions.\\n    Return the claims as a numbered list.\\n    \\n    Text:\\n    {text}\\n    \\n    Key Claims:\\n    ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_REGISTRY[PromptLoader.FETCH_KEY_CLAIMS.value]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "check-mate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
